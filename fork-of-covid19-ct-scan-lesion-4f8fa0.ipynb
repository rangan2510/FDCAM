{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torchmetrics==0.10.1","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:24:05.569668Z","iopub.execute_input":"2023-07-27T11:24:05.570061Z","iopub.status.idle":"2023-07-27T11:24:16.39239Z","shell.execute_reply.started":"2023-07-27T11:24:05.570027Z","shell.execute_reply":"2023-07-27T11:24:16.391137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport tqdm\nimport numpy as np\nfrom PIL import Image\nfrom skimage import io\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom torch.nn.modules.utils import _single, _pair, _triple\nfrom copy import deepcopy\nfrom sklearn.model_selection import train_test_split\nimport glob\nimport shutil\nimport torchmetrics","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:24:16.394953Z","iopub.execute_input":"2023-07-27T11:24:16.395668Z","iopub.status.idle":"2023-07-27T11:24:22.115144Z","shell.execute_reply.started":"2023-07-27T11:24:16.395625Z","shell.execute_reply":"2023-07-27T11:24:22.114142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 8\nNUM_EPOCHS = 75\nIMAGE_HEIGHT = 512\nIMAGE_WIDTH = 512\nIMAGE_MEAN = [0.2826, 0.2826, 0.2826]\nIMAGE_STD = [0.2695, 0.2695, 0.2695]\nLEARNING_RATE = 1e-4\nTRAIN_IMG_DIR = \"/kaggle/input/ra-attention-util-notebookf6efc8202c/frames/\"\nprint(\"Number of samples:\", len(os.listdir(TRAIN_IMG_DIR)))\ntrain_img_files, test_img_files = train_test_split(os.listdir(TRAIN_IMG_DIR), test_size=0.2, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:28:28.63431Z","iopub.execute_input":"2023-07-27T11:28:28.6348Z","iopub.status.idle":"2023-07-27T11:28:28.687997Z","shell.execute_reply.started":"2023-07-27T11:28:28.634754Z","shell.execute_reply":"2023-07-27T11:28:28.686854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SegmentationDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_files, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.images = image_files\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.image_dir, self.images[index])\n        mask_path = os.path.join(self.mask_dir, self.images[index])\n        image = io.imread(img_path)\n        mask = np.array(Image.open(mask_path).convert(\"L\"))\n        mask = (np.reshape(mask, (*mask.shape, 1))/255).astype(np.float32)\n        mask[mask>1.0] = 1.0\n        mask[mask<0.0] = 0.0\n\n        if self.transform:\n            augmentations = self.transform(image=image, mask=mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n            mask = mask.permute((2,0,1))\n\n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:31:21.173134Z","iopub.execute_input":"2023-07-27T11:31:21.173541Z","iopub.status.idle":"2023-07-27T11:31:21.184856Z","shell.execute_reply.started":"2023-07-27T11:31:21.173488Z","shell.execute_reply":"2023-07-27T11:31:21.183719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms(train=True):\n    train_transforms = []\n    if train:\n        train_transforms = [\n            A.Rotate(limit=15, p=0.5),\n        ]\n        \n    transform = A.Compose([\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        *train_transforms,\n        A.Normalize(mean=IMAGE_MEAN, std=IMAGE_STD),\n        ToTensorV2(),\n    ])\n    return transform\n\ntrain_dataset = SegmentationDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, train_img_files, get_transforms(True))\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\ntest_dataset = SegmentationDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, test_img_files, get_transforms(False))\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:31:32.059421Z","iopub.execute_input":"2023-07-27T11:31:32.060136Z","iopub.status.idle":"2023-07-27T11:31:32.070339Z","shell.execute_reply.started":"2023-07-27T11:31:32.060097Z","shell.execute_reply":"2023-07-27T11:31:32.068029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FuzzyConv(nn.Module):\n    \"\"\"\n    Fuzzy Conv Layer\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, mu=0.1, *args, **kwargs):\n        super(FuzzyConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.dilation = _pair(dilation)  # all pairs are considered to be of form (n, n)\n        self.mu = mu\n        if self.dilation[0] != self.dilation[1]:\n            raise NotImplementedError(f\"Unequal dilation not supported. Got dilation rates: {self.dilation}\")\n        if self.dilation[0] > 1:\n            fuzzy_points = np.linspace(self.mu, 1, int(self.dilation[0] / 2) + 1)\n            fuzzy_points /= np.sum(fuzzy_points)\n            self.fuzzy_points = nn.Parameter(torch.Tensor(fuzzy_points), requires_grad=True)\n            self.create_fuzzy_mask()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, dilation=self.dilation, *args, **kwargs)\n\n    def create_fuzzy_mask(self):\n        fh, fw = self.dilation if self.dilation[0] % 2 else (self.dilation[0] + 1, self.dilation[1] + 1)\n        fuzzy_mask = torch.zeros((fh, fw), device=self.fuzzy_points.device)\n        for e, v in enumerate(self.fuzzy_points):\n            for _ in range(int(self.dilation[0] / 2)):\n                fuzzy_mask[e : fh - e, e : fw - e] = v\n\n        self.fuzzy_mask = nn.Parameter(torch.broadcast_to(fuzzy_mask, (self.in_channels, 1, fh, fw)), requires_grad=False)\n\n    def forward(self, x):\n        if self.dilation[0] > 1:\n            if self.training:\n                self.create_fuzzy_mask()\n            x = F.conv2d(x, self.fuzzy_mask, padding=\"same\", groups=self.in_channels)\n        x = self.conv(x)\n        return x\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, in_channels, out_size, kernel_size, downsample_ratio, dilation, mu=0.1, upsample_mode=\"bilinear\"):\n        super(SpatialAttention, self).__init__()\n        self.downsample_ratio = downsample_ratio\n        if downsample_ratio == 1:\n            self.fuzzy_conv = FuzzyConv(in_channels, in_channels, kernel_size, dilation, bias=True, padding=dilation, stride=downsample_ratio, mu=mu)\n        else:\n            self.fuzzy_conv = FuzzyConv(in_channels, in_channels, kernel_size, dilation, bias=True, padding=dilation, stride=downsample_ratio, mu=mu) #  * (int(dilation / 2) + 1)\n            self.upsample = nn.Upsample(size=out_size, mode=upsample_mode)\n        \n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size, padding=\"same\") #  * (int(dilation / 2) + 1)\n\n    def forward(self, x):\n        if self.downsample_ratio == 1:\n            y = self.fuzzy_conv(x)\n        else:\n            y = self.fuzzy_conv(x)\n            y = self.upsample(y)\n        \n        y = self.conv2d(y)\n        return torch.add(x, y)\n\n\nclass GlobalLearnablePool(nn.Module):\n    \"\"\"\n    Learnable Pooling Layer.\n    Takes (n, ci, h, w) input and returns (n, co, 1, 1) output\n    \"\"\"\n\n    def __init__(self, in_channels, in_height, in_width):\n        super(GlobalLearnablePool, self).__init__()\n        self.in_channels = in_channels\n        self.pool_mask = nn.Parameter(torch.ones((in_channels, 1, in_height, in_width)) / (in_height * in_width), requires_grad=True)\n\n    def forward(self, x):\n        x = F.conv2d(x, self.pool_mask, groups=self.in_channels)\n        return x\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, in_height, in_width, se_hidden_channels, dropout_rate=0.1):\n        super(ChannelAttention, self).__init__()\n        self.pool = GlobalLearnablePool(in_channels, in_height, in_width)\n        self.linear1 = nn.Linear(in_channels, se_hidden_channels)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.linear2 = nn.Linear(se_hidden_channels, in_channels)\n\n    def forward(self, x):\n        out = self.pool(x)\n        out = out.reshape(x.shape[0], -1)\n        out = self.linear1(out)\n        out = F.relu(out)\n        out = self.dropout(out)\n        out = self.linear2(out)\n        out = out[:, :, None, None]  # reshape - add height and width axes\n        out = x * torch.sigmoid(out)\n        return out\n\nclass ChannelSpatialAttention(nn.Module):\n    def __init__(self, in_channels, out_size, kernel_size, downsample_ratio_list, dilation_rates_list, mu=0.1, upsample_mode=\"bilinear\"):\n        super().__init__()\n        if isinstance(downsample_ratio_list, int):\n            downsample_ratio_list = [downsample_ratio_list]\n        if isinstance(dilation_rates_list, int):\n            dilation_rates_list = [dilation_rates_list]\n        if isinstance(kernel_size, int):\n            kernel_size_list = [kernel_size] * len(dilation_rates_list)\n        else:\n            kernel_size_list = kernel_size\n        assert len(kernel_size_list) == len(downsample_ratio_list) == len(dilation_rates_list), \"Downsample list and Dilation list have to be equal length\"\n        fuzzy_convs = []\n        for kernel_size, stride, dilation in zip(kernel_size_list, downsample_ratio_list, dilation_rates_list):\n            fuzzy_convs.append(\n                nn.Sequential(\n                    ChannelAttention(in_channels, 32, 32, 512, 0.2),\n                    SpatialAttention(in_channels, out_size, kernel_size, stride, dilation, mu),\n                )\n            )\n        self.fuzzy_convs = nn.ModuleList(fuzzy_convs)\n        self.channel_conv = nn.Conv2d(in_channels * len(downsample_ratio_list), in_channels, (1, 1))\n\n    def forward(self, x):\n        out = []\n        for fuzzy_conv in self.fuzzy_convs:\n            out.append(fuzzy_conv(x))\n\n        out = torch.concat(out, dim=1)\n        out = self.channel_conv(out)\n        return out\n\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups) -> None:\n        super().__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        \"\"\"Channel Shuffle operation.\n        This function enables cross-group information flow for multiple groups\n        convolution layers.\n        Args:\n            x (Tensor): The input tensor.\n            groups (int): The number of groups to divide the input tensor\n                in the channel dimension.\n        Returns:\n            Tensor: The output tensor after channel shuffle operation.\n        \"\"\"\n\n        batch_size, num_channels, height, width = x.size()\n        assert num_channels % self.groups == 0, \"num_channels should be divisible by groups\"\n        channels_per_group = num_channels // self.groups\n\n        x = x.view(batch_size, self.groups, channels_per_group, height, width)\n        x = torch.transpose(x, 1, 2).contiguous()\n        x = x.view(batch_size, self.groups * channels_per_group, height, width)\n\n        return x\n\n\nclass ResidualDenseBlock(nn.Module):\n    \"\"\"Achieves densely connected convolutional layers.\n    `Densely Connected Convolutional Networks <https://arxiv.org/pdf/1608.06993v5.pdf>` paper.\n    Args:\n        channels (int): The number of channels in the input image.\n        growth_channels (int): The number of channels that increase in each layer of convolution.\n    \"\"\"\n\n    def __init__(self, channels: int, growth_channels: int) -> None:\n        super(ResidualDenseBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels + growth_channels * 0, growth_channels, (3, 3), (1, 1), (1, 1))\n        self.conv2 = nn.Conv2d(channels + growth_channels * 1, growth_channels, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(channels + growth_channels * 2, growth_channels, (3, 3), (1, 1), (1, 1))\n        self.conv4 = nn.Conv2d(channels + growth_channels * 3, growth_channels, (3, 3), (1, 1), (1, 1))\n        self.conv5 = nn.Conv2d(channels + growth_channels * 4, channels, (3, 3), (1, 1), (1, 1))\n        self.leaky_relu = nn.LeakyReLU(0.2, True)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out1 = self.leaky_relu(self.conv1(x))\n        out2 = self.leaky_relu(self.conv2(torch.cat([x, out1], 1)))\n        out3 = self.leaky_relu(self.conv3(torch.cat([x, out1, out2], 1)))\n        out4 = self.leaky_relu(self.conv4(torch.cat([x, out1, out2, out3], 1)))\n        out5 = self.conv5(torch.cat([x, out1, out2, out3, out4], 1))\n        out = torch.mul(out5, 0.2)\n        out = torch.add(out, x)\n        return out\n\n\nclass ResidualResidualDenseBlock(nn.Module):\n    \"\"\"Multi-layer residual dense convolution block.\n    Args:\n        channels (int): The number of channels in the input image.\n        growth_channels (int): The number of channels that increase in each layer of convolution.\n    \"\"\"\n\n    def __init__(self, channels: int, growth_channels: int) -> None:\n        super(ResidualResidualDenseBlock, self).__init__()\n        self.rdb1 = ResidualDenseBlock(channels, growth_channels)\n        self.rdb2 = ResidualDenseBlock(channels, growth_channels)\n        self.rdb3 = ResidualDenseBlock(channels, growth_channels)\n        self.batchnorm = nn.BatchNorm2d(channels)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out = self.rdb1(x)\n        out = self.rdb2(out)\n        out = self.rdb3(out)\n        # out = torch.mul(out, 0.2)\n        out = self.batchnorm(out)\n        out = torch.add(out, x)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:31:32.122269Z","iopub.execute_input":"2023-07-27T11:31:32.122578Z","iopub.status.idle":"2023-07-27T11:31:32.181902Z","shell.execute_reply.started":"2023-07-27T11:31:32.12255Z","shell.execute_reply":"2023-07-27T11:31:32.180966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.8)\n\ntrain_metrics = [torchmetrics.Accuracy().to(DEVICE), torchmetrics.Dice().to(DEVICE)] #, torchmetrics.classification.BinaryJaccardIndex().to(DEVICE)]\ntest_metrics = [torchmetrics.Accuracy().to(DEVICE), torchmetrics.Dice().to(DEVICE)] #, torchmetrics.classification.BinaryJaccardIndex().to(DEVICE)]","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:31:32.68493Z","iopub.execute_input":"2023-07-27T11:31:32.685363Z","iopub.status.idle":"2023-07-27T11:31:32.701492Z","shell.execute_reply.started":"2023-07-27T11:31:32.68532Z","shell.execute_reply":"2023-07-27T11:31:32.700547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_dice = 0\nmax_epoch = 0\nNUM_EPOCHS=5\nfor epoch in range(1, NUM_EPOCHS+1):\n    print(epoch,'/',NUM_EPOCHS)\n    total_loss = 0\n    model.train()\n    for imgs, masks in tqdm.notebook.tqdm(train_loader):\n        optimizer.zero_grad()\n        imgs = imgs.to(DEVICE)\n        masks = masks.to(DEVICE)\n        outputs = torch.sigmoid(model(imgs))\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        for metric in train_metrics:\n            metric.update(outputs, masks.to(torch.int8))\n\n    print(\"Train Loss:\", total_loss)\n    for metric in train_metrics:\n        print(metric, metric.compute().item())\n        metric.reset()\n    \n    print()\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for imgs, masks in tqdm.notebook.tqdm(test_loader):\n            imgs = imgs.to(DEVICE)\n            masks = masks.to(DEVICE)\n            outputs = torch.sigmoid(model(imgs))\n            loss = criterion(outputs, masks)\n            total_loss += loss.item()\n            for metric in test_metrics:\n                metric.update(outputs, masks.to(torch.int8))\n    \n    print(\"Test Loss:\", total_loss)\n    for metric in test_metrics:\n        print(metric, metric.compute().item())\n        if metric.__str__() == \"Dice()\" and metric.compute().item() > max_dice:\n            max_dice = metric.compute().item()\n            max_epoch = epoch\n            torch.save(model, f'model-{epoch}.pt')\n        metric.reset()\n    print()\n    \n    scheduler.step()\n\nprint(\"Max Dice:\", max_dice, \"Epoch:\", max_epoch)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:31:33.64592Z","iopub.execute_input":"2023-07-27T11:31:33.64905Z","iopub.status.idle":"2023-07-27T11:31:57.081098Z","shell.execute_reply.started":"2023-07-27T11:31:33.649003Z","shell.execute_reply":"2023-07-27T11:31:57.079331Z"},"trusted":true},"execution_count":null,"outputs":[]}]}